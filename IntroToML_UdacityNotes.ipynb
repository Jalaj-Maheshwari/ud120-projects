{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroToML_UdacityNotes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/vzmRMx3z8nVgA87K/aV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jalaj-Maheshwari/ud120-projects/blob/master/IntroToML_UdacityNotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NAIVE BAYES CLASSIFIER ALGORITHM**\n",
        "\n",
        "- Video : https://www.youtube.com/watch?v=b8M9CWxRyQ4 [Bayes Rule Algo via Cancer test Example]\n",
        "- Implementation : https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n"
      ],
      "metadata": {
        "id": "yqdpVJSwML37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FORESTS CLASSIFIER**\n",
        "\n",
        "- **Idea:** Creating a ensemble model (mixture of different decision trees where the average of the output is considered for determining the final class / classificationc class). Includes the concept of bagging (creating a bag of random dataset) and aggregation technique (aggregating the results of all bags)\n",
        "\n",
        "- Include creating a bootsrapped datasets by randomly selecting rows from dataset with sqrt(features) features in each of them and creating a decision tree for each of those datasets. Finally the aggregation of the results is performed to output an aggregated result. \n",
        "- Video : https://www.youtube.com/watch?v=v6VJ2RO66Ag\n",
        "- Number of default trees (n-estimators): 100 (scikit-learn), Ideal features to be considered = sqrt(no.of features)\n",
        "- Implementation : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "- ***Need to figure out***, how many of decision trees (predictors) is ideal count in random forests. \n"
      ],
      "metadata": {
        "id": "aaRD5YNEMHBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ADABOOST**\n",
        "\n",
        " - **Boosting** is a technique used to boost the efficiency of poorly performing datapoints wherein incorrectly predicted features from previous bag are fed to next bag to imporve its efficiency. \n",
        " - **Adaboost Idea**: Adaboost (Adaptive Boosting) is a type of boosting technique wherein the incorrectly predicted features from previous bag of dataset are picked to be trained as next bag of random dataset by checking the weights associated with the records inrelation to their error in their previous bag's result. So mostly likely all poor performing datapoints from previous bag are picked to be trained in the next bag along with other random datapoints (rows) selected. \n",
        " - **Approach**: Here decision tree stumps are used to create a bag. This is how the technique adapts itself by improving on previous incorrect outputs. Here the base learners are cerated using stomps as decision trees wherein a stomp is a decision tree with 2 or more leaf nodes. For each feature, a decision stump is created and then the decision stump with lowest entropy is selected as the first / next base learner to train the model on and check the results. Similarly, based on the results, the weights are assigned using some calculations and then required records (incorrect results) wrt weights are taken for next iteration of traning with next base learner. This is done till n learners are done (n specific initially). More about the formula and calculations part can be referred in below videos. Based on the User Guide of sklearn, the ideal number of estimators where the adaboost classifier can give optimal accuracy is ranging between 100 to 200 estimators. \n",
        "\n",
        " - Udacity Video link: https://www.youtube.com/watch?v=GM3CDQfQ4sw\n",
        " - Detailed Video: https://www.youtube.com/watch?v=NLRO1-jp5F8\n",
        " - Implementation : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboost#sklearn.ensemble.AdaBoostClassifier\n",
        " \n",
        "\n",
        "\n",
        "- **Cross-Validation-Score**: It is used to split the dataset in k-folds wherein at each iteration, k - 1 datapoints are used for training and the test is performed on remaining datapoints. Accuracy at step is calculated and finally an average is done for accuracy at k steps to determine the final accuracy. One of the usecase involves adaboost classifier. \n",
        "- More details about cross_val_score() : https://stackoverflow.com/questions/52611498/need-help-understanding-cross-val-score-in-sklearn-python\n"
      ],
      "metadata": {
        "id": "q3-2aPg8PV0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERAL NOTES:** \n",
        "\n",
        "- **Pickle file** is used to store python objects directly. \n",
        "\n",
        "- As obvious, the more the training data, the more the accuracy and lesser the overfitting. \n",
        "\n",
        "- In most cases, NaN values are replaced with 0. \n",
        "\n",
        "- Usually any random dataset needs to be converted in numpy array (usually by using np.array() function) to become compatible of being read by sklearn library. If its a normal dictionary or list, then it needs to be converted into a numpy array using method above. Refer feature_format.py under tools modules in this project's udacity repo. \n",
        "\n",
        "- Pick a continuous supervised learning algorithm when there is an order in the input dataset points. eg: Age, Income, Weight, etc. Whereas, always pick a discrete classifier when the order in the dataset does not exists / doesnt matter. eg: Phone number, Who wrote an email, multiple output / classes for a given scenario (eg: dog breed classification)\n",
        "\n",
        "- **Classfication vs Regression**: In C, we aim to derive a decision boundary whereas in R, we tend to find the best fit line for data. In C, we check the correctness using accuracy to predict a class whereas in R, we use R-squared or metric sto measure the dif between actual and predicted values. C -> Discrete Values (class labels) , R -> Continuous Values (numbers)\n",
        "More here: https://classroom.udacity.com/courses/ud120/lessons/2301748537/concepts/28680885390923\n",
        "\n",
        "- Always ensure to **remove the OUTLIER NOISE** from the training and testing set as it may effect our score. "
      ],
      "metadata": {
        "id": "Je1nnmEIyKOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LINEAR REGRESSION** \n",
        "\n",
        "- Simply used to solve a continuos regression problem using linear order regression where idea is to fit a line to all the training points in the training set using a basic equation of y = mx + c (m: slope, c: intercept) or\n",
        "y = w0 + w1 x1 (w0, w1: being the weights for the model, x1: being the feature)\n",
        "w0 is initially set to 0. \n",
        "\n",
        "- Implementation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "\n",
        "- Extracting regression model information - https://classroom.udacity.com/courses/ud120/lessons/2301748537/concepts/30101886230923\n",
        "\n",
        "- MSE (Mean squared error) and R-squared errors can be used to determine the efficiency of the model. \n",
        "\n",
        "- **Mean squared error (MSE)** is the average of sum of squared difference between actual value and the predicted or estimated value. It is also termed as mean squared deviation (MSD). **The lower the MSE, better the model is! **It generally determines whether the variance in prediction. In terms of linear regression, variance is a measure of how far observed values differ from the average of predicted values, i.e., their difference from the predicted value mean. Lesser the variance, better it is!\n",
        "\n",
        "- **R-Squared error** is the ratio of Sum of Squares Regression (SSR) and Sum of Squares Total (SST). Sum of Squares Regression is amount of variance explained by the regression line. R-squared value is used to measure the goodness of fit. **Greater the value of R-Squared (between 0 and 1), better is the regression model**. \n",
        "\n",
        "- Note that R2 is only bounded from below by 0 when evaluating a linear regression on its training set. If evaluated on a significantly different set, where the predictions of a regressor are worse than simply guessing the mean value for the whole set, the calculation of R2 can be negative.\n",
        "- R-squared = 1 - ((MSE) / Variance(y))\n",
        "\n",
        "- R-squared is generally preferred over (only) MSE for measurement of goodness of fit because value of MSE can keep increasing with larger datasets. Hence, no range bound can be defined for same. More about this here: https://classroom.udacity.com/courses/ud120/lessons/2301748537/concepts/24600085450923\n",
        "\n",
        "- Intuition behind why MSE is used: https://classroom.udacity.com/courses/ud120/lessons/2301748537/concepts/24668385410923\n",
        "- More about MSE and R-squared can be found here: https://vitalflux.com/mean-square-error-r-squared-which-one-to-use/\n",
        "\n",
        "- Take this exercise to understand what is a good fit under linear regression: https://classroom.udacity.com/courses/ud120/lessons/2301748537/concepts/24753685360923\n",
        "\n",
        "- **reg.score()** function is used to calcuate the r-sqaured metric for linear regression model wherein the function takes features_test and target_test (expected y) as input and checks the r-sqaured value by internally checking the prediction values for features_test and then checking its dif from the target_test (y) values. \n",
        "\n",
        "- **reg.coef_** is used to return the slope of the regression line generated after fitting the model on underlying training data. \n",
        "\n",
        "**Popular Algorithms used to achieve minimal MSE (best slope and intercept of line) in Linear regression:** \n",
        "\n",
        "- **Ordinary least sqaures (OLS)** : This is used by sklearn linear regression model function. \n",
        "\n",
        "- **Gradient Descent**\n",
        "\n",
        "**Why GD over OLS is preferred?**\n",
        "\n",
        "- Ideally, OLS works well for univariate dataset (ie 1 independent varaible y and 1 dependent variable x), whereas Gradient Descent (GD) works well for Multivariate dataset (ie 1 independent varaible y and multiple dependent variable x). This is because in GD, the computational complexity in case of multivariate dataset reduces a lot as compared to OLS (if considered for Multivariate dataset). OLS also works well when the dataset is smaller but GD works better when dataset is larger. Also, GD works on regularization techniques (brining down the weights of non-significant variables close to 0) ie. Generic Optimization. This is not done by OLS well.  \n",
        "- More about the diff here: https://www.youtube.com/watch?v=IyDwQNXDWns\n"
      ],
      "metadata": {
        "id": "snQEp__2Md7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MULTI-VARIATE REGRESSION**\n",
        "\n",
        "- **Idea:** Multiple variables involved in predicting the output. Sample exercise to get an idea of how this works: https://classroom.udacity.com/courses/ud120/lessons/2301748537/concepts/27818685480923"
      ],
      "metadata": {
        "id": "0OKG2TNSTW4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTLIER REMOVAL**\n",
        "\n",
        "- **Train the Algorithm and remove 10% of points with highest residual errors**\n",
        "(error between the actual and predicted values after the training) and retrain the algo of remaining data points. Iterate this process of traning and removing the points till we get the desired result. \n",
        "- Video: https://classroom.udacity.com/courses/ud120/lessons/2269418543/concepts/28641985670923"
      ],
      "metadata": {
        "id": "9cacMz-N6aTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNSUPERVISED LEARNING**\n",
        "\n",
        "- As the name suggests, it is used to find patterns in data with no labels. \n",
        "2 Popular supervised learning approaches are: **Clustering** and **Dimentionality Reduction** (reducing data points in 2D space in a 1D line)\n",
        "- More details about intro here: https://classroom.udacity.com/courses/ud120/lessons/c67d3714-d9ee-4a83-a0dc-5ffa17d97659/concepts/e534e19f-7739-4798-a81c-1ac8725b9b30"
      ],
      "metadata": {
        "id": "1XGK2ATkQqqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-MEANS CLUSTERING**\n",
        "\n",
        "**Overview:**\n",
        "- Is an iterative algorithm in which each observation belongs to the cluster with the nearest mean (centroids). The k-means algorithm captures the insight that each point in a cluster should be near to the center of that cluster. It works like this: first we choose k, the number of clusters we want to find in the data. Then, the centers of those k clusters, called centroids, are initialized in some fashion,\n",
        "\n",
        "- The algorithm then proceeds in two alternating parts: In the Reassign Points step, we assign every point in the data to the cluster whose centroid is nearest to it. In the Update Centroids step, we recalculate each centroid's location as the mean (center) of all the points assigned to its cluster. We then iterate these steps until the centroids stop moving, or equivalently until the points stop switching clusters.  \n",
        "\n",
        "- So precisely, it includes 2 steps : \n",
        "  **Assign and Optimize**, wherein in the assignment step a random centre is taken for the multiple clusters identified. NAd in optimize step, this centre is then moved so as to reduce the minimum distance from all the points in a cluster. The iteration of assign and optimize continues till the the centroid stop moving or the points stop switching clusters.  \n",
        "\n",
        "**Centroid initializaton strategies:** \n",
        "\n",
        "1. Randomly, \n",
        "2. Farthest point from last centroid. \n",
        "3. k-means++ where it selects a centroid with probability proportional to the square of its distance to the nearest preceding centroid. (learn more about this later)\n",
        "\n",
        "**Performance of k-means clustering:** \n",
        "- K-Means works best in datasets that have with clusters that are roughly equally-sized and shaped roughly regularly. So it works very well on the \"Gaussian Mixture\" data and the \"Packed Circles\" data\n",
        "\n",
        "- More about k-means clustering: http://www.naftaliharris.com/blog/visualizing-k-means-clustering/ [Visualising and learning how k-means clustering works]\n",
        "\n",
        "**Finding the optimal value of k for k-means:** \n",
        "\n",
        "- 2 methods can be used: **Elbow method** and **Silhuotte method** \n",
        "\n",
        "- **Elbow Method:** Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS becomes first starts to diminish\n",
        "\n",
        "- **Silhuotte method:** The silhouette value measures how similar a point is to its own cluster **(cohesion)(average of the distance of point i from other points)** compared to other clusters **(separation)(sum of distance of point i from all the other points j)**. **The range of the Silhouette value is between +1 and -1**. A high value is desirable and indicates that the point is placed in the correct cluster. If many points have a negative Silhouette value, it may indicate that we have created too many or too few clusters.\n",
        "\n",
        "- More about them below and calculation part: \n",
        "https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
        "\n",
        "- Technical Implementation in sklearn for K-Means Clustering: \n",
        "https://classroom.udacity.com/courses/ud120/lessons/c67d3714-d9ee-4a83-a0dc-5ffa17d97659/concepts/4be2767a-a694-427e-afa7-533559dc8beb\n",
        "\n",
        "- **NOTE:** K-Means clustering tend to provide different outputs on difference runs on different initialisations, even if the number of centroids and number of training points remain same throughout multiple runs. This is because K-Means output depends heavily on the way (methods used) to initialise its centroids in the beginning. \n",
        "\n",
        "**LOCAL MINIMUM IN K_MEANS:** \n",
        "\n",
        "- It usually means, that centroids can be sometimes be initialized in such a manner that they provide local minimum values (sub-optimal values) by dividing a cluster into multiple parts, which in reality could have been a whole cluster as a whole. Hence, this sometimes become a shortcoming / challenge in k-means. \n",
        "\n",
        "- For more understanding: https://classroom.udacity.com/courses/ud120/lessons/c67d3714-d9ee-4a83-a0dc-5ffa17d97659/concepts/e6da5453-a9a1-4fd0-bf3a-cb0f4a5c5767\n",
        "\n",
        "- https://classroom.udacity.com/courses/ud120/lessons/c67d3714-d9ee-4a83-a0dc-5ffa17d97659/concepts/540431b3-79a5-4441-9161-22f121900023\n",
        "\n",
        "- **How k-means works on more than 2 feature:** https://stackoverflow.com/questions/54861453/how-to-use-k-means-clustering-for-more-features [If you have more than 2 features, the k means clustering happens in n dimensional space where n is number of features.The number of dimensions in the vector of each sample would change and there is no need to change algorithm or approach]\n",
        "\n",
        "- (If needed) Plotting more than 2 features using plotpy tool: https://towardsdatascience.com/clustering-with-more-than-two-features-try-this-to-explain-your-findings-b053007d680a"
      ],
      "metadata": {
        "id": "VIuNEK5p08cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE SCALING**\n",
        "\n",
        "- **Feature Scaling is done to normalize the features in the dataset into a finite range.** In this process by which all the features are scaled up / down into a specific range (mostly between 0 and 1) so as to give equal balance to them (wrt numerical figures) such that one large numeric value of a feature does not dominates / influences the entire calculation process and the result. \n",
        "Refer example for more understanding: https://classroom.udacity.com/courses/ud120/lessons/fd5e3e6d-5485-41d3-b800-3be3844fac1f/concepts/43320810-ae8b-4524-b4d8-4cd069af540d\n",
        "\n",
        "- **Formula for feature scaling for a given feature in feature column using MinMaxScaler:** \n",
        "  - **X'(new rescaled feature) = (X - X_min) / (X_max - X_min)**\n",
        "\n",
        "- For reference: https://classroom.udacity.com/courses/ud120/lessons/fd5e3e6d-5485-41d3-b800-3be3844fac1f/concepts/cfd9a65b-ce1a-4fe3-ad68-c3443e023f5d\n",
        "\n",
        "- Try this to calculate a new scaled feature for a sample example: https://classroom.udacity.com/courses/ud120/lessons/fd5e3e6d-5485-41d3-b800-3be3844fac1f/concepts/b4391d52-222a-4631-b349-3bb97d3fdde3\n",
        "\n",
        "- **SkLearn implemenatation walkthrough for Min_Max_Scaler used for feature scaling:** https://classroom.udacity.com/courses/ud120/lessons/fd5e3e6d-5485-41d3-b800-3be3844fac1f/concepts/a8e0770c-7d73-48c0-b67a-ed603e22feb1\n",
        "\n",
        "- **IMP:** In case, **if all values of a feature are same, then X_max - X_min would result in 0**, leading to divide-by-zero error in feature scaling formula. Hence, in such a case, when all values for a feature are same, it is advisable to **directly assign 0.5** (halfway between 0 and 1) to all values of the features. \n",
        "\n",
        "- **MinMaxScaler** is one of the feature scaling algorithms demonstrated above. These are others used as well - https://www.analyticsvidhya.com/blog/2021/05/feature-scaling-techniques-in-python-a-complete-guide/ . **Normalization** and **Standardization** are the algorithms which are popularly used for feature scaling. Standardization is mostly used for normally distributed data like salary or age.\n",
        "\n",
        "- **IMP:** MinMaxScaler() handles NaN values. \n",
        "\n",
        "- **Difference between feature scaling and normalization:** \n",
        "The difference is that, in scaling, you're changing the **range** of your data while in normalization you're changing the **shape of the distribution of your data.** Hence normalization is an indirect type of feature scaling since it changes the shape of distribution of data and not changes the range. It is obviously less suceptible to outliers as it uses mean instead of min. \n",
        "\n",
        "- **Try this to check the understanding around when to use feature scaling:** \n",
        "https://classroom.udacity.com/courses/ud120/lessons/fd5e3e6d-5485-41d3-b800-3be3844fac1f/concepts/1b279729-2610-4e67-aa49-16aa40806fa9\n",
        "\n",
        "- **NOTE:** \n",
        "\n",
        " - **Outliers need to be taken care of before applying feature scaling**, else it can mess up the scaling process as X_min and X_max values are used in the process.  \n",
        "\n",
        " - **SVM with rbf kernel and K-Means are affected by feature rescaling** whereas Decision Trees and linear regressions arent impacted by feature rescaling. **Ideally, the algorithms were 2 dimensions affect the outcome (or the decision boundary) are affected by the feature rescaling.** DTrees make use of horizontal or vertical lines to make class boundaries and in Linear R, both coeff and feature associated goes hand in hand, so, even if feature is scaled, the coef's weigtage will remain unchanged and hence the decision boundary is not impacted. Refer here for more info: https://classroom.udacity.com/courses/ud120/lessons/fd5e3e6d-5485-41d3-b800-3be3844fac1f/concepts/02ddd119-f615-4ef6-b888-bcd4c52ac968 \n",
        "\n",
        " - **However, Linear Regression might sometimes be affected with feature scaling if gradient descent technique for linear regression is used**. Still a bit debatable. Do some more findings around this. \n",
        "\n",
        " - Try this to check the understanding around ignoring a feature: https://classroom.udacity.com/courses/ud120/lessons/1e7c8fc1-f736-4df2-9031-9f4f257b18f9/concepts/127d0a7a-328b-4f13-8d6a-c534abe5a7d1\n",
        "\n",
        "**UNIVARIATE FEATURE SELECTION**\n",
        "\n",
        "- This treats each feature independently and asks how much power it gives you in classifying or regressing. \n",
        " \n",
        "- **2 tools in sklearn for feature selection: SelectPercentiles and SelectKBest**\n",
        "\n",
        "- **SelectPercentile selects the X% of features that are most powerful (where X is a parameter) and SelectKBest selects the K features that are most powerful (where K is a parameter)**. \n",
        "\n",
        "- **Implementation for SelectPercentile**: \n",
        "  - https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html\n",
        "  - Explanation: https://www.youtube.com/watch?v=9zrQ_c5RZkI\n",
        "\n",
        "- **Implementation for SelectKBest:**\n",
        "  - https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
        "  - Explanation: https://www.youtube.com/watch?v=UW9U0bYJ-Ys (sort of how it works for f_regression criteri for continuous set of features and continuous target variable). **For f-regression criteion class, SelectKBest uses Pearsonr method and the use of underlying test for determining the relationship depends on the type of scoring fucntion you use.** \n",
        "\n",
        "- **SelectPercentile and SelectKBest uses different scoring functions such as f_class_if, f_regression, chi-square test for checking if a given feature is related to the output variable.** Following is the explanation for **chi-square tests**: \n",
        "https://www.youtube.com/watch?v=f53nXHoMXx4\n",
        "\n",
        "- **Implementation for TF-IdfVectorizer (Term freq - Inverse doc freq):** \n",
        "  - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "  - Explanation: https://www.youtube.com/watch?v=_RhHA_tYYXI\n",
        "\n",
        "- **TFIdVectorizer is bassically used for feature / text reduction in text learning sort of examples.** It works by removing the commonly occurring words in the documents by specifying a threshold (% of times a word occurs in the documents) (which if breached, the word gets removed from dataset). A clear candidate for feature reduction is text learning, since the data has such high dimension.\n",
        "\n",
        "**BIAS vs VARIANCE:** \n",
        "\n",
        "- **High Bias** [towards limited lesser features] **(Underfitting):**\n",
        "  - High bias situations are ones wherein little attention is given to data used. **A typical situation is using fewer features. A model is said to have high bias when it has high error on training data.** High bias usually leads to underfitting and using more features is recommended.   \n",
        "\n",
        "- **High Variance** [on smaller dataset] **(Overfitting):** \n",
        "  - High Variance means when the data is given too much importance and generalization is absent. **This leads to overfitting and usually happens due to smaller training dataset. To avoid high variance and overfitting, it is recommended to increase the training dataset and/or reduce the no. of features used. Incase where too many features lead to overfitting of the model and model does not generalize things well. Here, the error on test set is much higher than that on training set.**\n",
        "\n",
        "- **More details:** https://classroom.udacity.com/courses/ud120/lessons/1e7c8fc1-f736-4df2-9031-9f4f257b18f9/concepts/365e6a35-0341-4ad8-87cb-a6a804529b5b \n",
        "\n",
        "- **NOTE:** Sometimes we tend to use too many features to ensure that our model has minimum SSE (error) or inorder to avoid bias, we can use too many features which can lead to high variance. **So its imp to keep things balanced enough on how many no. of features to be used. Ideally we should only use those many features / fewer features that gives us Min SSE and high R2 values.** \n",
        "\n",
        "- More details: https://classroom.udacity.com/courses/ud120/lessons/1e7c8fc1-f736-4df2-9031-9f4f257b18f9/concepts/f96faf2a-0dc3-4df6-840b-7e65d6e1ee3d\n",
        "\n",
        "**REGULARIZATION**\n",
        "\n",
        "- **Regularization is a caoncept wherein the best fit between overfitting and underfitting is selected for the model.** It is a method for automatically penalizing the extra features. Some models perform the step of regulirization completely on their own. \n",
        "\n",
        "- Idea for regularization: https://classroom.udacity.com/courses/ud120/lessons/1e7c8fc1-f736-4df2-9031-9f4f257b18f9/concepts/0b7a91c7-0ebc-4443-bac8-b6b15d362be2\n",
        "\n",
        "- **One of the examples of regression model that does regularlization automatically is Lasso regression.** Lasso regression is slightly better than linear regression since it has a mechanism to penalize extra features in a manner than they add up to the SSE gained by overfitting earlier which, if was introduced due to use of too many features. Hence, this tends to avoid overfitting. \n",
        "\n",
        "- More about **Lasso Regression:** https://classroom.udacity.com/courses/ud120/lessons/1e7c8fc1-f736-4df2-9031-9f4f257b18f9/concepts/fd8bf032-861a-4072-a839-2f1605390805"
      ],
      "metadata": {
        "id": "qWQkbDPpH_3W"
      }
    }
  ]
}